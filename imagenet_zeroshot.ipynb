{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc8db8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbff5ef5-c4e0-408d-a48c-a4abb3fb9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/vla/scratch/micromamba/envs/VIT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import models.modeling as full_precision\n",
    "from models.modeling_nbitlinear import VisionTransformer, CONFIGS\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# from models.nbitlinear import NBitLinear, quant\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed08e47d-0146-49a2-8e7f-f2bffa4720f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 12 13:09:50 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   28C    P8             22W /  300W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037ff7ab-e454-446f-92db-3589cc1688d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2d46f9-544c-4ee7-a4c7-306ad91c34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"attention_data\", exist_ok=True)\n",
    "if not os.path.isfile(\"attention_data/ilsvrc2012_wordnet_lemmas.txt\"):\n",
    "    urlretrieve(\"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\", \"attention_data/ilsvrc2012_wordnet_lemmas.txt\")\n",
    "    \n",
    "imagenet_labels = dict(enumerate(open('attention_data/ilsvrc2012_wordnet_lemmas.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a729c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: run to download ViT pretrained-checkpoint \n",
    "# if not os.path.isfile(\"attention_data/ViT-B_16-224.npz\"):\n",
    "#     urlretrieve(\"https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\", \"attention_data/ViT-B_16-224.npz\")\n",
    "\n",
    "# pretrained_path = 'attention_data/ViT-B_16-224.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9468d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: or set path to checkpoint appropriately\n",
    "pretrained_path = '/fs/nexus-scratch/vla/ViT_pretrained_checkpoints/ViT-B_16-224.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed294f36-bfa4-4090-a190-79679001eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(size=256, interpolation=PIL.Image.BILINEAR),\n",
    "#     transforms.CenterCrop(size=(224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "# imagenet = datasets.ImageFolder(root=\"/fs/vulcan-datasets/imagenet/val\", transform=transform)\n",
    "# imagenet_loader = DataLoader(dataset=imagenet, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "272e90f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models.vision_transformer as vit\n",
    "\n",
    "transform = vit.ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3053717",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "imagenet = datasets.ImageFolder(root=\"/fs/vulcan-datasets/imagenet/val\", transform=transform)\n",
    "imagenet_loader = DataLoader(dataset=imagenet, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1212033-c84b-4260-ae8d-669fd40e5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    global_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(imagenet_loader):\n",
    "            images = images.to(device)\n",
    "            logits, _ = model(images)\n",
    "\n",
    "            for idx, image_logits in enumerate(logits):\n",
    "                probs = torch.nn.Softmax(dim=-1)(image_logits)\n",
    "                sorted_probs = torch.argsort(probs, dim=-1, descending=True)\n",
    "                        \n",
    "                y_hat_index = sorted_probs[0].item()\n",
    "                y_hat = imagenet_labels[y_hat_index]\n",
    "                        \n",
    "                y_index = labels[idx].item()\n",
    "                y = imagenet_labels[y_index]\n",
    "                        \n",
    "                if y_hat == y:\n",
    "                    global_acc += 1\n",
    "\n",
    "    global_acc /= len(imagenet_loader.dataset)\n",
    "    print(f\"acc: {global_acc}\")\n",
    "    \n",
    "    return global_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78b53730",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641a99c",
   "metadata": {},
   "source": [
    "## Full-Precision Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbc7715b-84c6-469a-b94e-270f22f310e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = full_precision.CONFIGS[\"ViT-B_16\"]\n",
    "\n",
    "baseline_model = full_precision.VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=False).to(device)\n",
    "baseline_model.load_from(np.load(pretrained_path))\n",
    "baseline_model.to(device)\n",
    "\n",
    "baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "457d15da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.70924\n",
      "Top-1 acc:0.70924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Baseline', 0.70924)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = eval_model(baseline_model)\n",
    "print(f'Top-1 acc:{acc}')\n",
    "gather.append(('Baseline', acc))\n",
    "gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225d749",
   "metadata": {},
   "source": [
    "## Absmax Quantized Models (PTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74df3df0-364c-4c8c-be5f-d27c2e36c704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): NBitLinear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): NBitLinear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): NBitLinear(in_features=768, out_features=768, bias=True)\n",
       "            (key): NBitLinear(in_features=768, out_features=768, bias=True)\n",
       "            (value): NBitLinear(in_features=768, out_features=768, bias=True)\n",
       "            (out): NBitLinear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_absmax_model(weight_bits=8, activation_bits=8):\n",
    "    config = CONFIGS[\"ViT-B_16\"]\n",
    "    config['weight_bits'] = weight_bits\n",
    "    config['activation_bits'] = activation_bits\n",
    "\n",
    "    model = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=False).to(device)\n",
    "    model.load_from(np.load(pretrained_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bit run\n",
    "model = load_absmax_model(8,8)\n",
    "acc = eval_model(model)\n",
    "print(f'Top-1 acc:{acc}')\n",
    "gather.append(('8-bit', acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc62d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-bit run\n",
    "model = load_absmax_model(6,6)\n",
    "acc = eval_model(model)\n",
    "print(f'Top-1 acc:{acc}')\n",
    "gather.append(('6-bit', acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit run\n",
    "model = load_absmax_model(4,4)\n",
    "acc = eval_model(model)\n",
    "print(f'Top-1 acc:{acc}')\n",
    "gather.append(('4-bit', acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4453cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-bit run\n",
    "model = load_absmax_model(2,2)\n",
    "acc = eval_model(model)\n",
    "print(f'Top-1 acc:{acc}')\n",
    "gather.append(('2-bit', acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
